## Chapter 2: Why Existing Approaches Fall Short

Before presenting VID, we should understand why you can't just apply existing methodologies to AI-augmented development. The failures aren't superficial — they're structural.

### Agile's Blind Spot

Agile's core insight is that requirements change, so we should plan in short iterations and adapt based on feedback. This remains valid. But Agile assumes the expensive operation is writing code, so its ceremonies optimize for coordination and prioritization of coding effort.

Standups ask: "What did you code yesterday? What will you code today?"

Sprint planning asks: "How much can we code this sprint?"

Retrospectives ask: "How can we code more effectively?"

Notice what's missing: systematic attention to verification.

Agile has testing, of course. But testing in Agile is typically focused on "does the feature work?" not "is this code trustworthy?" When code is written by humans who understand what they wrote, this is often sufficient. The developer's understanding provides implicit verification.

When code is generated by AI, that implicit verification disappears. The developer may not fully understand the generated code. "It passes tests" is not the same as "I understand why it's correct."

Agile provides no framework for deciding how much verification a piece of code needs, tracking where code came from (human versus AI), adjusting the development process based on code provenance, or building verification skills as a core competency. These gaps become critical when AI generates significant portions of your codebase.

### The "Just Add AI" Fallacy

Many teams try to integrate AI by simply adding it to their existing workflow:

"We do Scrum, but now developers use Copilot."

This approach fails because it treats AI as a faster typewriter rather than a fundamental shift in how code comes into existence. The methodology remains optimized for the old constraint while the actual constraint has changed.

The symptoms of this failure are predictable and often alarming. Developers generate more code than ever before, yet defect rates increase rather than decrease. Sprint velocity appears higher on paper, but production incidents rise correspondingly. Technical debt accumulates faster than before because generated code is accepted without deep understanding. Code reviews become rubber stamps—there's simply too much code to review carefully when AI multiplies output. Perhaps most dangerously, no one knows which parts of the codebase were AI-generated, making it impossible to apply appropriate scrutiny where it's needed most.

### The Vibe Coding Trap

On the opposite extreme, some developers abandon methodology entirely. "Vibe coding" — generating code through natural language prompts with minimal structure — produces impressive demos but dangerous production systems.

Vibe coding fails because:

**No verification criteria.** Without explicit criteria for correctness, there's no way to know if generated code is right. "It seems to work" is not verification.

**No risk awareness.** All code is treated equally, whether it's a utility function or an authentication handler.

**No learning loop.** Without systematic tracking, teams can't learn which patterns produce good outcomes.

**No accountability.** When everyone is prompting AI, no one is responsible for understanding the result.

Vibe coding works for prototypes and experiments. It's actively dangerous for production systems.

### Emerging AIDD Frameworks

Several "AI-Driven Development" frameworks have emerged attempting to address these gaps. They typically share these characteristics:

- Replace sprints with shorter cycles ("bolts")
- Emphasize specification as input to AI
- Add AI agents for various development tasks
- Provide prompt templates and workflows

These frameworks improve on naive AI adoption, but most share a fundamental flaw: **they optimize for generation velocity, not verification confidence**.

Critical questions remain unanswered: How do you know AI-generated code is correct? When should humans verify versus trust automation? How do you build verification skills in developers? How do you adapt verification intensity to different risk levels? Most fundamentally, how do you maintain understanding as AI writes increasingly more of your code?

VID addresses these questions directly.

---

